{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6e7f55",
   "metadata": {},
   "source": [
    "# Job Market Analysis — Step-by-step\n",
    "\n",
    "This notebook is created for learning: it walks through **data loading**, **cleaning**, **EDA**, **time-series aggregation**, **seasonal decomposition**, **forecasting (SARIMAX & RandomForest)**, and **exporting cleaned files** ready for Power BI.\n",
    "\n",
    "**How to use**: open this notebook in JupyterLab / Jupyter Notebook, run cells top-to-bottom. Explanations are included before each code cell.\n",
    "\n",
    "Files used/produced in this environment:\n",
    "- Input: `/mnt/data/ai_job_dataset.csv`\n",
    "- Outputs (saved by the notebook): `/mnt/data/ai_job_dataset_cleaned.csv` and `/mnt/data/ai_job_timeseries_weekly.csv`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0413fb",
   "metadata": {},
   "source": [
    "## 1) Load dataset\n",
    "Replace the path below if your file is elsewhere. We detect columns and display a quick sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt/data/ai_job_dataset.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.shape\n",
    "# show top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3fc05",
   "metadata": {},
   "source": [
    "## 2) Clean column names\n",
    "Lowercase, replace spaces with underscores and remove special characters. This simplifies later code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cf1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (df.columns.str.strip()\n",
    "                            .str.lower()\n",
    "                            .str.replace(' ', '_')\n",
    "                            .str.replace('[^0-9a-zA-Z_]', '', regex=True))\n",
    "    return df\n",
    "\n",
    "df = clean_column_names(df)\n",
    "print('Columns after cleaning:')\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75221c",
   "metadata": {},
   "source": [
    "## 3) Detect date & salary columns (best-effort)\n",
    "We look for common hints like 'date' or 'salary' and try to parse date-like columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_date_column(df):\n",
    "    hints = ['date','posted','created','published','post','listed']\n",
    "    for col in df.columns:\n",
    "        if any(h in col for h in hints):\n",
    "            parsed = pd.to_datetime(df[col], errors='coerce')\n",
    "            if parsed.notna().mean() > 0.3:\n",
    "                return col\n",
    "    # fallback: highest parseable fraction\n",
    "    best = None; best_frac=0\n",
    "    for col in df.columns:\n",
    "        parsed = pd.to_datetime(df[col], errors='coerce')\n",
    "        frac = parsed.notna().mean()\n",
    "        if frac>best_frac:\n",
    "            best_frac=frac; best=col\n",
    "    return best if best_frac>0.5 else None\n",
    "\n",
    "def guess_salary_column(df):\n",
    "    hints = ['salary','ctc','package','pay','compensation']\n",
    "    for col in df.columns:\n",
    "        if any(h in col for h in hints):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "DATE_COL = guess_date_column(df)\n",
    "SALARY_COL = guess_salary_column(df)\n",
    "print('Guessed date col:', DATE_COL)\n",
    "print('Guessed salary col:', SALARY_COL)\n",
    "if DATE_COL:\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\n",
    "    print('Non-null dates:', df[DATE_COL].notna().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a85ac4",
   "metadata": {},
   "source": [
    "## 4) Parse salary column (best-effort)\n",
    "This function converts common salary formats into numeric values (handles k, ranges, LPA/lakh, currency symbols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def parse_salary_text(s):\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    if isinstance(s,(int,float)) and not math.isnan(s):\n",
    "        return float(s)\n",
    "    text = str(s).lower().replace(',','').replace('$','').replace('inr','').replace('rs','').replace('₹','')\n",
    "    if 'lpa' in text or 'lakh' in text or 'lac' in text:\n",
    "        nums = [float(''.join(ch for ch in t if (ch.isdigit() or ch=='.'))) for t in text.replace('-', ' ').split() if any(c.isdigit() for c in t)]\n",
    "        if nums:\n",
    "            return np.mean(nums) * 100000\n",
    "    if 'k' in text:\n",
    "        parts = text.split('-')\n",
    "        nums = []\n",
    "        for p in parts:\n",
    "            n = ''.join(ch for ch in p if (ch.isdigit() or ch=='.'))\n",
    "            if n:\n",
    "                nums.append(float(n))\n",
    "        if nums:\n",
    "            return np.mean(nums) * 1000\n",
    "    if '-' in text or 'to' in text:\n",
    "        sep = '-' if '-' in text else 'to'\n",
    "        parts = [p.strip() for p in text.split(sep)]\n",
    "        nums = []\n",
    "        for p in parts:\n",
    "            n = ''.join(ch for ch in p if (ch.isdigit() or ch=='.'))\n",
    "            if n:\n",
    "                nums.append(float(n))\n",
    "        if nums:\n",
    "            return np.mean(nums)\n",
    "    num = ''.join(ch for ch in text if (ch.isdigit() or ch=='.'))\n",
    "    try:\n",
    "        return float(num) if num else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "if SALARY_COL:\n",
    "    df['_salary_parsed'] = df[SALARY_COL].apply(parse_salary_text)\n",
    "    df['_salary_parsed'].describe()\n",
    "else:\n",
    "    print('No salary column detected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab49587",
   "metadata": {},
   "source": [
    "## 5) Basic text cleaning (job title, company, location)\n",
    "Lowercase and strip whitespace to standardize categories so aggregations are cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['job_title','job','title','company','location','company_name','company_location']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str).str.strip().str.replace('\\n',' ').str.replace('\\r',' ').str.lower()\n",
    "\n",
    "# Show top job titles\n",
    "for c in ['job_title','title','job']:\n",
    "    if c in df.columns:\n",
    "        print('\\nTop job titles:')\n",
    "        print(df[c].value_counts().head(10))\n",
    "        break\n",
    "\n",
    "if 'company' in df.columns:\n",
    "    print('\\nTop companies:')\n",
    "    print(df['company'].value_counts().head(10))\n",
    "\n",
    "if 'location' in df.columns:\n",
    "    print('\\nTop locations:')\n",
    "    print(df['location'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57527cfb",
   "metadata": {},
   "source": [
    "## 6) Missing values & decisions\n",
    "We display columns with missing values and decide simple imputation/drop strategies (explain choices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca821ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = df.isna().sum().sort_values(ascending=False)\n",
    "miss[miss>0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7ae80",
   "metadata": {},
   "source": [
    "## 7) Time-series aggregation\n",
    "Aggregate posting counts weekly and monthly from the detected date column. These series will be used for decomposition & forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATE_COL:\n",
    "    df_dates = df.dropna(subset=[DATE_COL]).set_index(DATE_COL)\n",
    "    ts_weekly = df_dates.resample('W').size().rename('job_count')\n",
    "    ts_monthly = df_dates.resample('M').size().rename('job_count')\n",
    "    display(ts_weekly.head())\n",
    "    plt.plot(ts_weekly.index, ts_weekly.values); plt.title('Weekly job counts'); plt.show()\n",
    "else:\n",
    "    print('No date column detected; create one before running time-series steps.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505308ff",
   "metadata": {},
   "source": [
    "## 8) Seasonal decomposition\n",
    "If the weekly series is long enough, decompose into trend/seasonality/residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63caab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ts_weekly' in globals() and len(ts_weekly)>=24:\n",
    "    tsw = ts_weekly.asfreq('W').fillna(0)\n",
    "    res = seasonal_decompose(tsw, period=52, model='additive', extrapolate_trend='freq')\n",
    "    res.plot(); plt.show()\n",
    "else:\n",
    "    print('Weekly series too short for decomposition (need ~24+ weeks).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7219b",
   "metadata": {},
   "source": [
    "## 9) Forecasting\n",
    "We attempt two approaches:\n",
    "- SARIMAX (statistical model)\n",
    "- RandomForest with lag features (machine learning model)\n",
    "\n",
    "We evaluate with RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04311f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return {'rmse':rmse,'mae':mae}\n",
    "\n",
    "# SARIMAX\n",
    "if 'ts_weekly' in globals() and len(ts_weekly)>=24:\n",
    "    tsw = ts_weekly.asfreq('W').fillna(0)\n",
    "    n_test = min(12, max(4, int(len(tsw)*0.2)))\n",
    "    train = tsw.iloc[:-n_test]; test = tsw.iloc[-n_test:]\n",
    "    model = SARIMAX(train, order=(1,1,1), seasonal_order=(0,0,0,0), enforce_stationarity=False, enforce_invertibility=False)\n",
    "    sarima_res = model.fit(disp=False)\n",
    "    pred = sarima_res.get_forecast(steps=len(test))\n",
    "    pred_mean = pred.predicted_mean\n",
    "    plt.plot(train.index, train.values, label='train'); plt.plot(test.index, test.values, label='test'); plt.plot(pred_mean.index, pred_mean.values, label='pred'); plt.legend(); plt.show()\n",
    "    print('SARIMAX eval:', evaluate(test.values, pred_mean.values))\n",
    "else:\n",
    "    print('Not enough data for SARIMAX')\n",
    "\n",
    "# RandomForest with lag features\n",
    "if 'ts_weekly' in globals() and len(ts_weekly)>=24:\n",
    "    tsw = ts_weekly.asfreq('W').fillna(0)\n",
    "    df_lag = tsw.to_frame('y')\n",
    "    n_lags = 12\n",
    "    for lag in range(1,n_lags+1):\n",
    "        df_lag[f'lag_{lag}'] = df_lag['y'].shift(lag)\n",
    "    df_lag = df_lag.dropna()\n",
    "    n_test = min(12, max(4, int(len(df_lag)*0.2)))\n",
    "    train = df_lag.iloc[:-n_test]; test = df_lag.iloc[-n_test:]\n",
    "    X_train = train.drop(columns=['y']); y_train = train['y']\n",
    "    X_test = test.drop(columns=['y']); y_test = test['y']\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    plt.plot(y_test.index, y_test.values, label='actual'); plt.plot(y_test.index, y_pred, label='pred'); plt.legend(); plt.show()\n",
    "    print('RF eval:', evaluate(y_test.values, y_pred))\n",
    "else:\n",
    "    print('Not enough data for ML forecasting')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ccdd4",
   "metadata": {},
   "source": [
    "## 10) Save cleaned datasets for Power BI\n",
    "Save the cleaned dataset and weekly timeseries CSV for direct import into Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ed83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_PATH = '/mnt/data/ai_job_dataset_cleaned.ipynb_placeholder.csv'\n",
    "TS_PATH = '/mnt/data/ai_job_timeseries_weekly.csv'\n",
    "# Save cleaned dataframe (CSV)\n",
    "df.to_csv('/mnt/data/ai_job_dataset_cleaned.csv', index=False)\n",
    "print('Saved cleaned CSV to /mnt/data/ai_job_dataset_cleaned.csv')\n",
    "# Save weekly timeseries if exists\n",
    "if 'ts_weekly' in globals():\n",
    "    ts_weekly.reset_index().to_csv(TS_PATH, index=False)\n",
    "    print('Saved weekly timeseries to', TS_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297b6e9",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes: Power BI tips\n",
    "1. Import the cleaned CSV via **Get data → Text/CSV**.\n",
    "2. In Power Query set `posting_date` type to *Date* and Close & Apply.\n",
    "3. Use **Line chart** with date on the axis (choose Month/Week) and job count as value.\n",
    "4. Use the Analytics pane → Forecast to add an interactive forecast for presentations.\n",
    "\n",
    "---\n",
    "\n",
    "End of notebook. Happy learning!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
